{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f31a3de3",
   "metadata": {},
   "source": [
    "# Chapter 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b89a91e",
   "metadata": {},
   "source": [
    "## Handle the missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe7a813",
   "metadata": {},
   "source": [
    "Missing Values causes many problems and these are some of the methods to get rid of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d99f7f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.dropna(subset=[\"total_bedrooms\"], inplace=True)\n",
    "#Dropping the missing values\n",
    "\n",
    "housing.drop(\"total_bedrooms\", axis=1)\n",
    "#Dropping the column with missing values\n",
    "\n",
    "median = housing[\"total_bedrooms\"].median() # option 3\n",
    "housing[\"total_bedrooms\"].fillna(median, inplace=True)\n",
    "# This last method is called imputing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc15ed73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "#You can change the strategy based on your data\n",
    "\n",
    "\"\"\"\n",
    "Missing values can also be replaced with the mean value\n",
    "(strategy=\"mean\"), or with the most frequent value\n",
    "(strategy=\"most_frequent\"), or with a constant value\n",
    "(strategy=\"constant\", fill_value=…). The last two strategies\n",
    "support non-numerical data.\n",
    "\"\"\"\n",
    "\n",
    "housing_num = housing.select_dtypes(include=[np.number])\n",
    "#Choosing the numerical columns to train it with them\n",
    "\n",
    "imputer.fit(housing_num)\n",
    "\n",
    "X = imputer.transform(housing_num)\n",
    "#Imputing the missing values in \"housing_num\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273adc8a",
   "metadata": {},
   "source": [
    "* **KNN Imputer :** replaces each missing value with the mean of the k-nearest\n",
    "  neighbors’ values for that feature. The distance is based on all the available features.\n",
    "\n",
    "\n",
    "* **Iterative Imputer :** trains a regression model per feature to predict the\n",
    "  missing values based on all the other available features. It then trains the model\n",
    "  again on the updated data, and repeats the process several times, improving the\n",
    "  models and the replacement values at each iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23797c96",
   "metadata": {},
   "source": [
    "## Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0318f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "housing_cat_encoded = ordinal_encoder.fit_transform(housing_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e91d635",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "cat_encoder = OneHotEncoder()\n",
    "housing_cat_1hot = cat_encoder.fit_transform(housing_cat)\n",
    "\n",
    "\"\"\"\"\n",
    "you can set sparse=False when creating the\n",
    "OneHotEncoder, in which case the transform() method will return a\n",
    "regular (dense) NumPy array directly.\n",
    "\"\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c374b62",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "        <b>Warning :\n",
    "        As with all estimators, it is important to fit the scalers to the training data only: never\n",
    "        use fit() or fit_transform() for anything else than the training set. Once you\n",
    "        have a trained scaler, you can then use it to transform() any other set, including the\n",
    "        validation set, the test set, and new data. Note that while the training set values will\n",
    "        always be scaled to the specified range, if new data contains outliers, these may end up\n",
    "        scaled outside the range. If you want to avoid this, just set the clip hyperparameter toTrue.\n",
    "        </b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177badc3",
   "metadata": {},
   "source": [
    "## Rescaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e847fbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "min_max_scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "housing_num_min_max_scaled = min_max_scaler.fit_transform(housing_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6c3c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "std_scaler = StandardScaler()\n",
    "housing_num_std_scaled = std_scaler.fit_transform(housing_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344d972d",
   "metadata": {},
   "source": [
    "**What to do When a feature’s distribution has a heavy tail ?**\n",
    "* you should first transform it to shrink the heavy tail, and if\n",
    "  possible to make the distribution roughly symmetrical.\n",
    "  \n",
    "* replace the feature with its square root (or raise the feature to a power between 0 and 1).\n",
    "\n",
    "* If the feature has a really long and heavy tail, such as a\n",
    "  power law distribution, then replacing the feature with its logarithm may help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e550e8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "log_transformer = FunctionTransformer(np.log,\n",
    "inverse_func=np.exp)\n",
    "log_pop = log_transformer.transform(housing[[\"population\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a9f813",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"\n",
    "(RBF)—any function that depends only on the distance between the input\n",
    "value and a fixed point. The most commonly used RBF is the Gaussian\n",
    "RBF\n",
    "\"\"\"\"\n",
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "age_simil_35 = rbf_kernel(housing[[\"housing_median_age\"]],\n",
    "[[35]], gamma=0.1)\n",
    "\n",
    "\"\"\"\n",
    "shows this new feature as a function of the housing median age\n",
    "(solid line). It also shows what the feature would look like if you used a\n",
    "smaller gamma value.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ef6b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "target_scaler = StandardScaler()\n",
    "scaled_labels = target_scaler.fit_transform(housing_labels.to_frame())\n",
    "#Scaling\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(housing[[\"median_income\"]], scaled_labels)\n",
    "#Modeling\n",
    "\n",
    "some_new_data = housing[[\"median_income\"]].iloc[:5] \n",
    "# pretend this is new data\n",
    "\n",
    "scaled_predictions = model.predict(some_new_data)\n",
    "#Prediction\n",
    "\n",
    "predictions = target_scaler.inverse_transform(scaled_predictions)\n",
    "#The Inverse scaling transform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390c7280",
   "metadata": {},
   "source": [
    "## Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da978df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "(\"impute\", SimpleImputer(strategy=\"median\")),\n",
    "(\"standardize\", StandardScaler()),\n",
    "])\n",
    "\n",
    "housing_num = num_pipeline.fit_transform(housing_num)\n",
    "\n",
    "# This an example of a basic pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0a2fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "num_pipeline = make_pipeline(SimpleImputer(strategy=\"median\"),\n",
    "StandardScaler())\n",
    "\n",
    "housing_num = num_pipeline.fit_transform(housing_num)\n",
    "\n",
    "# This an example of a basic pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6601bfa",
   "metadata": {},
   "source": [
    "## Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba6bf15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "tree_rmses = -cross_val_score(tree_reg, housing, housing_labels,scoring=\"neg_root_mean_squared_error\", cv=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6231788",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e15008e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "forest_reg = make_pipeline(preprocessing,\n",
    "RandomForestRegressor(random_state=42))\n",
    "forest_rmses = -cross_val_score(forest_reg, housing,\n",
    "housing_labels,\n",
    "scoring=\"neg_root_mean_squared_error\", cv=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9c691c",
   "metadata": {},
   "source": [
    "## Fine-Tune Your Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afce1f5",
   "metadata": {},
   "source": [
    "**1- Grid Search**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04917442",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "full_pipeline = Pipeline([\n",
    "(\"preprocessing\", preprocessing),\n",
    "(\"random_forest\", RandomForestRegressor(random_state=42)),\n",
    "])\n",
    "\n",
    "param_grid = [\n",
    "{'preprocessing__geo__n_clusters': [5, 8, 10],\n",
    "'random_forest__max_features': [4, 6, 8]},\n",
    "{'preprocessing__geo__n_clusters': [10, 15],\n",
    "'random_forest__max_features': [6, 8, 10]},\n",
    "]\n",
    "\n",
    "grid_search = GridSearchCV(full_pipeline, param_grid, cv=3,scoring='neg_root_mean_squared_error')\n",
    "\n",
    "grid_search.fit(housing, housing_labels)\n",
    "\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65309c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_res = pd.DataFrame(grid_search.cv_results_)\n",
    "cv_res.sort_values(by=\"mean_test_score\", ascending=False,inplace=True)\n",
    "rmse = -score\n",
    "cv_res.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0c14a6",
   "metadata": {},
   "source": [
    "**2- Randomized Search**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e26c54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "\n",
    "param_distribs = {'preprocessing__geo__n_clusters':\n",
    "randint(low=3, high=50),\n",
    "'random_forest__max_features': randint(low=2,\n",
    "high=20)}\n",
    "\n",
    "rnd_search = RandomizedSearchCV(\n",
    "full_pipeline, param_distributions=param_distribs, n_iter=10,\n",
    "cv=3,\n",
    "scoring='neg_root_mean_squared_error', random_state=42\n",
    ")\n",
    "\n",
    "rnd_search.fit(housing, housing_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12f6cf6",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "        <b>Scikit-Learn also has HalvingRandomSearchCV and\n",
    "HalvingGridSearchCV hyperparameter search classes. Their goal is to\n",
    "use the computational resources more efficiently, either to train faster or to\n",
    "explore a larger hyperparameter space.</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe66f12",
   "metadata": {},
   "source": [
    "## Analyzing the Best Models and Their Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96f30ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = rnd_search.best_estimator_ \n",
    "# includes preprocessing\n",
    "\n",
    "feature_importances = final_model[\"random_forest\"].feature_importances_\n",
    "feature_importances.round(2)\n",
    "\n",
    "# Let’s sort these importance scores in descending order and display them next to their corresponding attribute names:\n",
    "sorted(zip(feature_importances,final_model[\"preprocessing\"].get_feature_names_out()),reverse=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
